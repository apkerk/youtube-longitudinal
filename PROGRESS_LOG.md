# Progress Log: YouTube Longitudinal Data Collection

**Purpose:** Chronological record of all work completed on this project  
**Update Instructions:** Add new entries at the TOP of each month's section

---

## Current Status (as of Feb 16, 2026)

**Phase:** Phase 1 of Gender Gap Panel â€” Infrastructure built, test-verified, ready for production
**Roadmap Position:** All collection infrastructure built. Need Katie's approval for full enumeration + daily runs.
**Data Quality Status:** Bailey's xlsx cleaned to 14,169 unique channels (515 duplicates removed, race typos corrected). Three canonical output files produced.
**Next Steps:**
1. Get Katie's approval to run full video enumeration (~28,338 API units, ~48 min)
2. After enumeration: first full daily_stats run (~29,000 API units/day)
3. Set up launchd automation for daily 3 AM EST runs
4. Run AI Creator Census when ready (~500,000 API units)

---

## Feb 2026

### Feb 16, 2026 â€” 07:21 PM [Infrastructure Slide Deck â€” Complete]
- Built 10-slide LaTeX Beamer deck (`output/youtube-longitudinal-infrastructure-deck.tex`) documenting the full data collection infrastructure
- Custom theme: navy/amber/slate palette, 16:9 aspect ratio, progressive reveal overlays
- Covers: two research programs, panel composition (14,169 channels with gender/race breakdown), three-dataset architecture (TikZ flow diagram), schemas with sample data rows, 5-stream sampling design, AI Creator Census (17 terms + 3 research designs), future enrichment pipeline, quota budget (29,300 units/day = 2.9%)
- Full deck-compile protocol: 3 compilation passes, second-agent narrative review, third-agent graphics audit
- Final PDF: 19 pages (with overlays), 0 overfull hbox/vbox warnings
- Fixes applied from reviews: woman % 24.4â†’24.5 (rounding), "parallel"â†’"same run" on TikZ arrow, clarified 7-field schema label
- What's next: Katie reviews deck; production runs still awaiting approval

### Feb 16, 2026 â€” 07:15 PM [Status Review + Slide Deck Prompt]
- Confirmed all 4 agents completed and all scripts test-verified
- Wrote agent prompt for infrastructure overview slide deck (Beamer, 10-12 slides)
- Identified next session priorities: (1) full video enumeration, (2) first daily stats run, (3) launchd automation, (4) AI Creator Census
- What's next: Run enumeration in new session, then set up daily automation

### Feb 16, 2026 â€” 06:41 PM [Gender Gap Panel â€” Full Infrastructure Build]
- Built complete gender gap panel infrastructure via 4-agent parallel strategy (A: data prep, B: API infra, C: collection scripts, D: AI census)
- **Data prep (Agent A):**
  - Created `src/collection/clean_baileys.py` â€” parses Bailey's xlsx with openpyxl header-based lookup, fixes 6 race typos, deduplicates 515 duplicate rows â†’ 14,169 unique channels
  - Produced 3 output files: `data/processed/gender_gap_panel_clean.csv` (30 cols), `data/channels/gender_gap/channel_ids.csv`, `data/channels/gender_gap/channel_metadata.csv`
  - Updated `src/config.py` with 9 new paths, AI_SEARCH_TERMS (17 terms), 4 new schemas, get_daily_panel_path() helper
- **API infrastructure (Agent B):**
  - Added `get_all_video_ids()` to youtube_api.py â€” full playlist pagination with checkpoint/resume
  - Added `get_video_stats_batch()` â€” lean video stats fetch (part="statistics" only)
  - Added quota tracking to `execute_request()` â€” logs to data/logs/quota_YYYYMMDD.csv (backward compatible)
  - Created `__init__.py` for panels/, enrichment/, analysis/ modules
- **Collection scripts (Agent C):**
  - Created `src/collection/enumerate_videos.py` â€” builds video inventory with checkpoint/resume, UCâ†’UU playlist conversion
  - Created `src/panels/daily_stats.py` â€” DailyStatsCollector class with 4-step pipeline (video stats â†’ channel stats â†’ save â†’ new video detection)
- **AI census (Agent D):**
  - Created `src/collection/discover_ai_creators.py` â€” video-first search across 17 AI terms, 12-month time windows, order="relevance"
- **Test verification results (all passing):**
  - clean_baileys: 14,169 rows, all validations pass, race typos corrected
  - enumerate_videos --test --limit 2: 1,038 videos from 2 channels, correct 5-col schema
  - daily_stats --test: 250 video stats + 2 channel stats, daily panel files correct
  - discover_ai_creators --test --limit 5: 107 AI channels found
  - Backward compatibility: discover_intent.py and sweep_channels.py still import correctly
  - Quota tracking: data/logs/quota_20260216.csv being written
- Installed all Python deps for Python 3.14: openpyxl, pyyaml, pandas, tqdm, isodate, google-api-python-client
- What's next: Katie approves full video enumeration run, then start daily panel collection

### Feb 16, 2026 â€” 04:30 PM [AI Design Integration â€” Planning & Scope Expansion]
- Read and evaluated design document (`SECOND_BRAIN/03-research/YOUTUBE_DATASET_DESIGN.md`) â€” three new research designs: AI Creator Census, AI Adoption Diffusion Panel, Audience Response
- Audited all existing Python scripts against proposed architecture. Mapped reusable code vs. gaps.
- Analyzed both raw data files: confirmed 14,169 unique channels, 100% overlap between Infludata and Bailey's lists
- **Corrected misdiagnosis:** Bailey's xlsx does NOT have 4,097 misaligned rows. Those channels have BLANK gender/race (uncoded). Apparent misalignment was from parsing xlsx cells positionally instead of by cell reference.
- **SCOPE EXPANSION (Katie-approved):** This repo now owns longitudinal data collection on the 14,169 gender gap channels. Gender gap paper analysis stays in CH2. Updated CLAUDE.md.
- Resolved all 9 methodological decisions with Katie: all 14,169 channels, broad AI search terms, keyword-first AI detection, randomized comment sampling, partitioned CSVs to start
- Created new directory structure: `src/panels/`, `src/enrichment/`, `src/analysis/`, `data/channels/gender_gap/`, `data/channels/ai_census/`, `data/video_inventory/`, `data/daily_panels/{video_stats,channel_stats}/`, `data/transcripts/`, `data/comments/`, `logs/`
- Wrote full implementation plan: `.claude/plans/cached-knitting-puffin.md`
- Wrote agent handoff document: `docs/AGENT_HANDOFF.md`
- What's next: Hand off to parallel agents for implementation (data prep, API infrastructure, collection scripts, AI census)

### Feb 16, 2026 â€” 02:15 PM [Project Restructuring]
- Flattened project from triple-nested `youtube-longitudinal/youtube-longitudinal/youtube-longitudinal/` to root-level layout
- Production code (`src/`) moved from Level 3 to root; all `Path(__file__)` relative paths verified working
- Created project CLAUDE.md with session protocol, safety rules, coding standards
- Added `.claude/rules/` (01-session-protocol, 02-data-collection)
- Added `.claude/skills/log-update/` for session-end commit+push workflow
- Moved 4 existing analysis skills (stata-regression, r-econometrics, python-panel-data, referee-audit) to root `.claude/skills/`
- Detailed docs from Level 2 promoted to root (PROJECT_MASTER_PLAN, PROGRESS_LOG, TECHNICAL_SPECS, DECISION_LOG)
- Reference docs (API variable ref, sampling experiments, quota analysis) moved to `docs/`
- Updated `.gitignore` (comprehensive: data, secrets, artifacts, archive, output)
- Archived: v1 legacy scripts, superseded rule files (AI_RULES, ANTIGRAVITY_RULES, MY_WORKFLOW), loose conversation/template files, stray 656MB .dta file (confirmed duplicate of dissertation copy)
- Removed nested `.git/` repository (history captured in commit message)
- Resolved merge conflict with remote (b527cb0 production commit)
- What's next: install Python deps, then start production collection

### Feb 02, 2026 â€” 05:30 PM (Late Evening Session)
**Session Focus:** Full Production Pipeline Implementation

**Work Completed:**
- **Complete Rewrite:** Built production-grade modular architecture with 15 new files
- **Configuration System:** Created `config.py` with comprehensive constants:
  - 8-language keyword mappings (Intent + Non-Intent)
  - Full YouTube topic hierarchy (200+ topics with decoding)
  - Stream-specific directories and targets
  - Data schemas for channels, videos, sweeps
- **Enhanced API Module:** Upgraded `youtube_api.py` with:
  - Full topic extraction and decoding (hierarchical topics)
  - Comprehensive channel details (28 fields for initial, 8 for sweeps)
  - Video batch fetching with Shorts classification
  - New video detection for longitudinal tracking
  - Search helpers with pagination
- **Collection Scripts (5):**
  - `discover_intent.py` â€” Stream A: Intent creators (200k target)
  - `discover_non_intent.py` â€” Stream A': Non-intent creators (200k target)
  - `discover_benchmark.py` â€” Stream B: Algorithm favorites (2k)
  - `discover_random.py` â€” Stream C: Random prefix (50k)
  - `discover_casual.py` â€” Stream D: Casual uploaders (25k)
- **Sweep System:**
  - `sweep_channels.py` â€” Main sweep with checkpoint/resume
  - `detect_new_videos.py` â€” New video detection logic
- **Validation:**
  - `validate_sweep.py` â€” Data quality checks (duplicates, anomalies, policy changes)
- **Testing:** 
  - Stream A test: 62 Hindi channels collected successfully
  - Stream D test: 15 casual channels collected successfully
  - Validation: 0 errors, all systems operational

**Sample Size Decision:**
- Updated Stream A and A' from 25k â†’ **200k each** to handle attrition
- Rationale: Early-stage creator dropout is 50-70%; need huge buffer
- Expected English channels: 50k-80k per stream (sufficient for analysis)

**Quota Impact:**
- Initial collection (2 streams, 400k channels): ~808k units (fits in 1 day)
- Recommendation: Use `--skip-first-video` flag for speed
- Can enrich with first video data later if needed

**Directory Structure Created:**
```
youtube-longitudinal/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ youtube_api.py
â”‚   â”œâ”€â”€ collection/
â”‚   â”‚   â”œâ”€â”€ discover_intent.py
â”‚   â”‚   â”œâ”€â”€ discover_non_intent.py
â”‚   â”‚   â”œâ”€â”€ discover_benchmark.py
â”‚   â”‚   â”œâ”€â”€ discover_random.py
â”‚   â”‚   â””â”€â”€ discover_casual.py
â”‚   â”œâ”€â”€ sweeps/
â”‚   â”‚   â”œâ”€â”€ sweep_channels.py
â”‚   â”‚   â””â”€â”€ detect_new_videos.py
â”‚   â””â”€â”€ validation/
â”‚       â””â”€â”€ validate_sweep.py
â””â”€â”€ data/
    â”œâ”€â”€ channels/
    â”‚   â”œâ”€â”€ stream_a/
    â”‚   â”œâ”€â”€ stream_a_prime/
    â”‚   â”œâ”€â”€ stream_b/
    â”‚   â”œâ”€â”€ stream_c/
    â”‚   â””â”€â”€ stream_d/
    â”œâ”€â”€ videos/
    â””â”€â”€ logs/
```

**Files Created:**
- `src/config.py` (700+ lines)
- `src/youtube_api.py` (rewritten, 1000+ lines)
- `src/collection/discover_intent.py`
- `src/collection/discover_non_intent.py`
- `src/collection/discover_benchmark.py`
- `src/collection/discover_random.py`
- `src/collection/discover_casual.py`
- `src/sweeps/sweep_channels.py`
- `src/sweeps/detect_new_videos.py`
- `src/validation/validate_sweep.py`
- Module `__init__.py` files (3)

**Key Design Features:**
- **Topic Support:** Full YouTube hierarchical topics (not just categories)
- **Language Tracking:** `discovery_language` field tracks which language found each channel
- **Shock Readiness:** All policy-relevant fields included (made_for_kids, privacy_status, etc.)
- **Checkpoint/Resume:** Sweeps can be interrupted and resumed
- **Validation:** Automated quality checks for data integrity

**Next Steps (Tomorrow):**
1. Start Stream A collection (200k channels, ~404k units)
2. Start Stream A' collection (200k channels, ~404k units)
3. Consider using `--skip-first-video` for initial speed
4. Set up sweep schedule once initial collection complete

---

### Feb 02, 2026 â€” 04:30 PM (Evening Session)
**Session Focus:** Comprehensive Sampling Methodology Validation & Quad-Stream Design

**Work Completed:**
- **Experiment Battery:** Ran 6 systematic experiments testing sampling strategies:
  - EXP-001/003: Bias Profile Comparison (Vowels vs Raw vs Random Prefix)
  - EXP-002/010: New Creator Yield Rates (Intent vs Non-Intent keywords)
  - EXP-005: Channel ID Enumeration (result: not viable)
  - EXP-006: Language Bias Detection (8 languages tested)
  - EXP-007: Region Code Impact
  - EXP-008: Pagination Depth Bias
- **Critical Discovery:** English-only keywords miss 86% of findable new creators. Hindi has highest yield (35.3%).
- **Design Evolution:** Upgraded from Triple-Stream to Quad-Stream design with Stream D (Amateur Baseline).
- **Documentation Created:**
  - `SAMPLING_EXPERIMENTS.md` â€” Full experiment log with results
  - `QUOTA_ANALYSIS.md` â€” Sample size and polling frequency analysis
  - `test_sampling_battery.py` â€” Comprehensive test script
  - `test_language_pagination.py` â€” Language and pagination experiments
  - `discover_amateur.py` â€” Stream D collection script
  - `discover_cohort_multilingual.py` â€” Enhanced 8-language cohort discovery

**Key Findings:**
| Finding | Evidence | Implication |
|---------|----------|-------------|
| Vowel search massively biased | 1.16M median views, 94% big channels | Relabeled as "Algorithm Favorites" |
| English misses 86% of creators | 38 vs 271 across 8 languages | MUST use multilingual |
| Hindi highest yield | 35.3% vs 30.9% English | Prioritize Hindi |
| Pagination doesn't help | Page 2 higher views than Page 1 | Abandon strategy |
| Polling is cheap | 130k channels = 2,600 units/day | Daily polling feasible |

**Quota Analysis Summary:**
- Recommended panel: 130,000 channels (50k A + 10k B + 50k C + 20k D)
- Daily polling cost: 2,600 units (0.26% of quota)
- Collection budget: 500,000 units/day
- 80%+ quota remains for flexibility/experiments

**Files Created/Modified:**
- `SAMPLING_EXPERIMENTS.md` (Created)
- `QUOTA_ANALYSIS.md` (Created)
- `TECHNICAL_SPECS.md` (Major update â€” Quad-Stream)
- `PROJECT_MASTER_PLAN.md` (Updated findings & design)
- `src/test_sampling_battery.py` (Created)
- `src/test_language_pagination.py` (Created)
- `src/discover_amateur.py` (Created)
- `src/discover_cohort_multilingual.py` (Created)
- `config/config.yaml` (Created with API key)

**Next Steps (Production Ramp-Up):**
1. Run 24-hour multilingual cohort collection
2. Run amateur baseline collection
3. Set up automated daily polling
4. Design hybrid polling tiers (hot/warm/cold)

---

### Feb 02, 2026 â€” 03:00 PM
**Session Focus:** Methodological Pivot to Triple-Stream Design

**Work Completed:**
- **Bias Diagnostics:** Ran empirical tests comparing "Vowel Search" (Median Views 652k) vs "Random Prefix" (Median Views 22).
- **Sampling Strategy:** Validated that a single "Random" sample is insufficient.
- **Design Pivot:** Adopted "Triple-Stream" approach (Intentional A + Visible B + Deep Random C).
- **Scripting:** Created `discover_cohort.py` (Stream A) and `discover_random.py` (Stream B initial version).
- **Documentation:** Consolidated artifacts into `PROJECT_MASTER_PLAN.md` and `TECHNICAL_SPECS.md`.

**Key Findings:**
- **"Filing for Videos" (Zhou et al.):** Confirmed as reference for Random Prefix Sampling.
- **Quota Capacity:** We have effectively infinite quota (1M/day) for this scale, allowing us to run all three streams simultaneously.

**Decisions Made:**
- **Triple-Stream Design:** (See `DECISION_LOG.md` entry 001). Chosen to control for survivorship bias while checking market competitiveness.
- **Vowel Rotation:** Used for Stream B to ensure linguistic coverage of the "Visible" market.

**Files Created/Modified:**
- `PROJECT_MASTER_PLAN.md`: Created (Canonical roadmap).
- `TECHNICAL_SPECS.md`: Created (Canonical specs).
- `DECISION_LOG.md`: Created (Canonical decision record).
- `src/test_bias_deep_dive.py`: Created (Diagnostic tool).

**Next Steps (Immediate):**
1. Clean up redundant documentation (`implementation_plan.md`, `sampling_methodology.md`).
2. Finalize script names.

### Feb 02, 2026 â€” 03:15 PM
**Session Focus:** Deep Review of Sampling Strategy & Script Logic

**Work Completed:**
- **Strategy Deep Dive:** Conducted literature review on "Snowball Sampling" vs "Random Prefix" vs "External Lists." Created `sampling_strategy_review.md`.
- **Code Audit:** Analyzed `discover_cohort.py` and `discover_visible.py` to document exact search keywords and query logic (`q` searches snippets, not just titles).
- **Validation:** Confirmed "Stream C" (Deep Random) logic: inherently captures 99% nonsense to prove the 1% signal is distinct.
- **Cleanup:** Renamed `discover_random.py` -> `discover_visible.py` to clarify its role as "Market Baseline."
- **Deleted:** Redundant artifacts (`task.md`, `implementation_plan.md`, etc.) after consolidation.

### Feb 02, 2026 â€” 03:30 PM
**Session Focus:** Project Setup & Git Initialization

**Work Completed:**
- **Git Initialization:** Initialized local repository and created `.gitignore` to exclude sensitive/large files.
- **GitHub Link:** Successfully linked local repo to `apkerk/youtube-longitudinal` and pushed `main` branch.
- **Documentation Audit:** Created missing standard files (`MY_WORKFLOW.md`, `writing-patterns.md`, `deck.md`) from templates to ensure full compliance with the research system.
- **Folder Structure:** created `drafts/` and `archive/` directories.

**Files Created/Modified:**
- `.gitignore`: Created.
- `MY_WORKFLOW.md`: Created.
- `writing-patterns.md`: Created.
- `deck.md`: Created.

**Next Steps (Immediate):**
1. ðŸ›‘ USER ACTION REQUIRED: Run the 3-step validation suite to confirm network access.
2. Set up `launchd` for daily automation.

**Key Findings:**
- **Stream A Keywords:** Validated list (`Welcome`, `Intro`, `Vlog 1`) targets intentionality.
- **Stream C Logic:** Confirmed that "Nonsense" results are a *feature*, not a bug, providing a true Zero Baseline.
- **Alternatives Rejected:** "Snowball Sampling" rejected due to Homophily Bias (echo chambers).

**Files Created/Modified:**
- `sampling_strategy_review.md`: Created (Comparative analysis).
- `src/discover_visible.py`: Renamed and updated docstrings.
- `src/discover_deep_random.py`: Created.

**Next Steps (Immediate):**
1. ðŸ›‘ USER ACTION REQUIRED: Run the 3-step validation suite to confirm network access.
2. Set up `launchd` for daily automation.
